'use client';

import React, { useState, useRef, useCallback } from 'react';
import { Button } from '@/components/ui/Button';
import { cn } from '@/lib/utils';

interface VoiceRecorderProps {
  onTranscriptionComplete: (text: string) => void;
  className?: string;
}

export function VoiceRecorder({ onTranscriptionComplete, className }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [error, setError] = useState<string>('');
  const [useFallback, setUseFallback] = useState(false);
  const [isUploading, setIsUploading] = useState(false);
  const [uploadProgress, setUploadProgress] = useState(0);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const recognitionRef = useRef<any>(null);
  const fileInputRef = useRef<HTMLInputElement | null>(null);

  // æ£€æŸ¥æ˜¯å¦æ”¯æŒWeb Speech API
  const isWebSpeechSupported = () => {
    return 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;
  };

  // ä½¿ç”¨Web Speech APIè¿›è¡Œè¯­éŸ³è¯†åˆ«
  const startWebSpeechRecognition = useCallback(() => {
    if (!isWebSpeechSupported()) {
      setError('æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½');
      return;
    }

    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    
    recognition.lang = 'zh-CN';
    recognition.continuous = true;
    recognition.interimResults = false;
    recognition.maxAlternatives = 1;

    recognition.onstart = () => {
      console.log('ğŸ¤ Web Speech API è¯­éŸ³è¯†åˆ«å¼€å§‹');
      setIsRecording(true);
      setRecordingTime(0);
      
      // å¼€å§‹è®¡æ—¶
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
    };

    recognition.onresult = (event: any) => {
      const transcript = event.results[event.results.length - 1][0].transcript;
      const confidence = event.results[event.results.length - 1][0].confidence;
      
      console.log('âœ… Web Speech API è¯†åˆ«ç»“æœ:', transcript);
      console.log('ğŸ¯ ç½®ä¿¡åº¦:', confidence);
      
      onTranscriptionComplete(transcript);
      setIsRecording(false);
      setIsProcessing(false);
      setRecordingTime(0);
      
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    };

    recognition.onerror = (event: any) => {
      console.error('âŒ Web Speech API é”™è¯¯:', event.error);
      setError(`è¯­éŸ³è¯†åˆ«å¤±è´¥: ${event.error}`);
      setIsRecording(false);
      setIsProcessing(false);
      
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    };

    recognition.onend = () => {
      console.log('ğŸ”š Web Speech API è¯†åˆ«ç»“æŸ');
      setIsRecording(false);
      setIsProcessing(false);
      
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    };

    recognitionRef.current = recognition;
    recognition.start();
  }, [onTranscriptionComplete]);

  // å¤„ç†æ–‡ä»¶ä¸Šä¼ 
  const handleFileUpload = useCallback(async (file: File) => {
    try {
      setError('');
      setIsUploading(true);
      setUploadProgress(0);

      // éªŒè¯æ–‡ä»¶ç±»å‹
      const allowedTypes = ['audio/wav', 'audio/mp3', 'audio/mpeg', 'audio/webm', 'audio/ogg'];
      if (!allowedTypes.includes(file.type) && !file.name.match(/\.(wav|mp3|mpeg|webm|ogg)$/i)) {
        throw new Error('ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  WAVã€MP3ã€WEBM æˆ– OGG æ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶');
      }

      // éªŒè¯æ–‡ä»¶å¤§å° (é™åˆ¶ä¸º50MB)
      if (file.size > 50 * 1024 * 1024) {
        throw new Error('æ–‡ä»¶å¤ªå¤§ï¼Œè¯·ä¸Šä¼ å°äº50MBçš„éŸ³é¢‘æ–‡ä»¶');
      }

      console.log('ğŸ“ å¼€å§‹å¤„ç†ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶:', {
        name: file.name,
        size: file.size,
        type: file.type
      });

      // åˆ›å»ºFormDataç”¨äºæ–‡ä»¶ä¸Šä¼ 
      const formData = new FormData();
      formData.append('audio', file);
      formData.append('uploadType', 'file'); // æ ‡è®°ä¸ºæ–‡ä»¶ä¸Šä¼ 

      setUploadProgress(30);

      // è°ƒç”¨è¯­éŸ³è¯†åˆ«API
      const response = await fetch('/api/voice-recognition', {
        method: 'POST',
        body: formData
      });

      setUploadProgress(80);

      const result = await response.json();
      
      if (!response.ok) {
        throw new Error(result.error || result.details || 'æ–‡ä»¶ä¸Šä¼ å¤±è´¥');
      }

      if (!result.success) {
        throw new Error(result.error || 'è¯­éŸ³è¯†åˆ«å¤±è´¥');
      }

      console.log('âœ… æ–‡ä»¶ä¸Šä¼ è¯­éŸ³è¯†åˆ«æˆåŠŸ:', result.text);
      setUploadProgress(100);
      
      // è°ƒç”¨å›è°ƒå‡½æ•°
      onTranscriptionComplete(result.text);
      
      // é‡ç½®çŠ¶æ€
      setIsUploading(false);
      setUploadProgress(0);
      
    } catch (err) {
      console.error('æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥:', err);
      const errorMessage = err instanceof Error ? err.message : 'æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·é‡è¯•';
      setError(errorMessage);
      setIsUploading(false);
      setUploadProgress(0);
      
      // å¦‚æœç§‘å¤§è®¯é£APIå¤±è´¥ï¼Œä¸éœ€è¦é™çº§åˆ°Web Speech APIï¼ˆæ–‡ä»¶ä¸Šä¼ æ¨¡å¼ï¼‰
      // å› ä¸ºWeb Speech APIä¸æ”¯æŒå¤„ç†éŸ³é¢‘æ–‡ä»¶
    }
  }, [onTranscriptionComplete]);

  // è§¦å‘æ–‡ä»¶é€‰æ‹©
  const triggerFileUpload = useCallback(() => {
    fileInputRef.current?.click();
  }, []);

  // æ–‡ä»¶é€‰æ‹©å¤„ç†
  const handleFileChange = useCallback((event: React.ChangeEvent<HTMLInputElement>) => {
    const file = event.target.files?.[0];
    if (file) {
      handleFileUpload(file);
    }
    // æ¸…ç©ºinputå€¼ï¼Œå…è®¸é‡å¤ä¸Šä¼ åŒä¸€æ–‡ä»¶
    if (event.target) {
      event.target.value = '';
    }
  }, [handleFileUpload]);

  const startRecording = useCallback(async () => {
    try {
      setError('');
      
      // é…ç½®éŸ³é¢‘å‚æ•°ä»¥ç¬¦åˆç§‘å¤§è®¯é£è¦æ±‚
      const constraints = {
        audio: {
          sampleRate: 16000,      // 16kHzé‡‡æ ·ç‡
          channelCount: 1,        // å•å£°é“
          echoCancellation: true, // å›éŸ³æ¶ˆé™¤
          noiseSuppression: true, // å™ªéŸ³æŠ‘åˆ¶
          autoGainControl: true   // è‡ªåŠ¨å¢ç›Šæ§åˆ¶
        }
      };
      
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // å°è¯•ä½¿ç”¨æŒ‡å®šçš„MIMEç±»å‹
      let mimeType = 'audio/wav';
      if (MediaRecorder.isTypeSupported('audio/wav; codecs=pcm')) {
        mimeType = 'audio/wav; codecs=pcm';
      } else if (MediaRecorder.isTypeSupported('audio/webm; codecs=opus')) {
        mimeType = 'audio/webm; codecs=opus';
      }
      
      console.log('ğŸ™ï¸ å½•éŸ³é…ç½®:', { mimeType, constraints });
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      
      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };
      
      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(chunksRef.current, { type: mimeType });
        await processAudio(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };
      
      mediaRecorder.start();
      setIsRecording(true);
      setRecordingTime(0);
      
      // å¼€å§‹è®¡æ—¶
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
      
    } catch (err) {
      setError('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
      console.error('Error accessing microphone:', err);
    }
  }, []);

  const stopRecording = useCallback(() => {
    if (recognitionRef.current && useFallback) {
      // åœæ­¢Web Speech API
      recognitionRef.current.stop();
      console.log('ğŸ›‘ åœæ­¢Web Speech APIå½•éŸ³');
    } else if (mediaRecorderRef.current && isRecording) {
      // åœæ­¢ä¼ ç»Ÿå½•éŸ³
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      setIsProcessing(true);
      
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    }
  }, [isRecording, useFallback]);

  const processAudio = async (audioBlob: Blob) => {
    try {
      // éªŒè¯å›è°ƒå‡½æ•°æ˜¯å¦å­˜åœ¨
      if (typeof onTranscriptionComplete !== 'function') {
        console.error('onTranscriptionComplete is not a function:', onTranscriptionComplete);
        setError('å›è°ƒå‡½æ•°é…ç½®é”™è¯¯');
        setIsProcessing(false);
        return;
      }

      console.log('ğŸ¤ å¼€å§‹å¤„ç†å½•éŸ³éŸ³é¢‘ï¼Œä½¿ç”¨FormDataä¸Šä¼ ...');
      
      console.log('ğŸ“Š éŸ³é¢‘ä¿¡æ¯:', {
        originalSize: audioBlob.size,
        audioType: audioBlob.type,
        duration: recordingTime
      });
      
      // å¯¹äºå½•éŸ³æ–‡ä»¶ï¼Œä½¿ç”¨FormDataæ–¹å¼ä¸Šä¼ ä»¥æ”¯æŒæ›´å¤§çš„æ–‡ä»¶
      const formData = new FormData();
      
      // åˆ›å»ºFileå¯¹è±¡
      const audioFile = new File([audioBlob], `recording-${Date.now()}.wav`, {
        type: audioBlob.type || 'audio/wav'
      });
      
      formData.append('audio', audioFile);
      formData.append('uploadType', 'recording'); // æ ‡è®°ä¸ºå½•éŸ³ä¸Šä¼ 
      formData.append('duration', recordingTime.toString());

      // è°ƒç”¨è¯­éŸ³è¯†åˆ«API
      const response = await fetch('/api/voice-recognition', {
        method: 'POST',
        body: formData
      });

      const result = await response.json();
      
      if (!response.ok) {
        throw new Error(result.error || result.details || 'è¯­éŸ³è¯†åˆ«è¯·æ±‚å¤±è´¥');
      }

      console.log('ğŸ“¡ è¯­éŸ³è¯†åˆ«APIå“åº”:', result);
      
      // å¦‚æœè¿”å›çš„æ˜¯å…·ä½“çš„APIé”™è¯¯ï¼Œæ˜¾ç¤ºæ›´å‹å¥½çš„ä¿¡æ¯
      if (result && !result.success && result.error) {
        console.log('ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:', result.error);
      }

      if (!result.success) {
        throw new Error(result.error || 'è¯­éŸ³è¯†åˆ«å¤±è´¥');
      }

      const recognizedText = result.text;
      console.log('âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸ:', recognizedText);
      console.log('ğŸ¯ ç½®ä¿¡åº¦:', result.confidence);
      
      if (!recognizedText || recognizedText.trim().length === 0) {
        throw new Error('æœªè¯†åˆ«åˆ°è¯­éŸ³å†…å®¹ï¼Œè¯·é‡æ–°å½•åˆ¶');
      }
      
      console.log('ğŸ¤ æ­£åœ¨è°ƒç”¨å›è°ƒå‡½æ•°:', typeof onTranscriptionComplete);
      onTranscriptionComplete(recognizedText);
      
      setIsProcessing(false);
      setRecordingTime(0);
      
    } catch (err) {
      console.error('è¯­éŸ³è½¬æ¢å¤±è´¥:', err);
      const errorMessage = err instanceof Error ? err.message : 'è¯­éŸ³è½¬æ¢å¤±è´¥ï¼Œè¯·é‡è¯•';
      setError(errorMessage);
      setIsProcessing(false);
      
      // å¦‚æœç§‘å¤§è®¯é£APIè°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°Web Speech API
      if (errorMessage.includes('é…ç½®ç¼ºå¤±') || 
          errorMessage.includes('è°ƒç”¨å¤±è´¥') || 
          errorMessage.includes('CloudBase') ||
          errorMessage.includes('not found') ||
          errorMessage.includes('éŸ³é¢‘æ ¼å¼') ||
          errorMessage.includes('éŸ³é¢‘é‡‡æ ·ç‡')) {
        
        console.log('ğŸ”„ ç§‘å¤§è®¯é£APIè°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨Web Speech API');
        setUseFallback(true);
        setError('ç§‘å¤§è®¯é£APIè°ƒç”¨å¤±è´¥ï¼Œå·²åˆ‡æ¢åˆ°æµè§ˆå™¨è¯­éŸ³è¯†åˆ«');
        
        // è‡ªåŠ¨é‡æ–°å¼€å§‹å½•éŸ³ï¼Œä½¿ç”¨Web Speech API
        setTimeout(() => {
          setError('');
          startWebSpeechRecognition();
        }, 1000);
      }
    }
  };

  const formatTime = (seconds: number): string => {
    const minutes = Math.floor(seconds / 60);
    const remainingSeconds = seconds % 60;
    return `${minutes.toString().padStart(2, '0')}:${remainingSeconds.toString().padStart(2, '0')}`;
  };

  return (
    <div className={cn('space-y-4', className)}>
      <div className="flex items-center space-x-4">
        {!isRecording ? (
          <>
            <Button
              onClick={startRecording}
              disabled={isProcessing || isUploading}
              className="flex items-center space-x-2"
            >
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
                <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
              </svg>
              <span>{isProcessing ? 'å¤„ç†ä¸­...' : 'å¼€å§‹å½•éŸ³'}</span>
            </Button>
            
            <Button
              onClick={triggerFileUpload}
              disabled={isProcessing || isUploading}
              variant="outline"
              className="flex items-center space-x-2"
            >
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
                <path d="M14,2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18A2,2 0 0,0 20,20V8L14,2M18,20H6V4H13V9H18V20Z"/>
              </svg>
              <span>{isUploading ? 'ä¸Šä¼ ä¸­...' : 'ä¸Šä¼ æ–‡ä»¶'}</span>
            </Button>
          </>
        ) : (
          <Button
            onClick={stopRecording}
            variant="destructive"
            className="flex items-center space-x-2"
          >
            <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
              <path d="M6 6h12v12H6z"/>
            </svg>
            <span>åœæ­¢å½•éŸ³</span>
          </Button>
        )}
        
        {(isRecording || isProcessing || isUploading) && (
          <div className="flex items-center space-x-2">
            <div className={cn(
              'w-3 h-3 rounded-full',
              isRecording ? 'bg-red-500 animate-pulse' : 
              isUploading ? 'bg-blue-500 animate-pulse' : 'bg-yellow-500'
            )} />
            <span className="text-sm text-gray-600">
              {isRecording ? `å½•éŸ³ä¸­ ${formatTime(recordingTime)}` : 
               isUploading ? `ä¸Šä¼ ä¸­ ${uploadProgress}%` : 'è½¬æ¢ä¸­...'}
            </span>
          </div>
        )}
      </div>
      
      {/* éšè—çš„æ–‡ä»¶è¾“å…¥ */}
      <input
        ref={fileInputRef}
        type="file"
        accept="audio/wav,audio/mp3,audio/mpeg,audio/webm,audio/ogg,.wav,.mp3,.mpeg,.webm,.ogg"
        onChange={handleFileChange}
        style={{ display: 'none' }}
      />
      
      {/* ä¸Šä¼ è¿›åº¦æ¡ */}
      {isUploading && uploadProgress > 0 && (
        <div className="w-full bg-gray-200 rounded-full h-2">
          <div 
            className="bg-blue-600 h-2 rounded-full transition-all duration-300" 
            style={{ width: `${uploadProgress}%` }}
          />
        </div>
      )}
      
      {error && (
        <div className="text-red-600 text-sm bg-red-50 p-2 rounded">
          {error}
        </div>
      )}
      
      <div className="text-xs text-gray-500 space-y-1">
        <div>
          â€¢ <strong>å½•éŸ³æ¨¡å¼</strong>ï¼šç‚¹å‡»"å¼€å§‹å½•éŸ³"å¹¶å…è®¸æµè§ˆå™¨è®¿é—®éº¦å…‹é£æƒé™
        </div>
        <div>
          â€¢ <strong>æ–‡ä»¶ä¸Šä¼ </strong>ï¼šç‚¹å‡»"ä¸Šä¼ æ–‡ä»¶"é€‰æ‹©WAVã€MP3ã€WEBMæˆ–OGGæ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆæœ€å¤§50MBï¼‰
        </div>
        <div>
          â€¢ åœ¨å®‰é™ç¯å¢ƒä¸‹æ¸…æ™°åœ°è¯´è¯ï¼Œå»ºè®®å½•éŸ³æ—¶é•¿10-60ç§’ï¼Œæ”¯æŒæ›´é•¿å½•éŸ³çš„æŒä¹…åŒ–å¤„ç†
        </div>
        <div>
          â€¢ {useFallback ? 'ä½¿ç”¨æµè§ˆå™¨Web Speech APIè¿›è¡Œè¯­éŸ³è¯†åˆ«' : 'é€šè¿‡CloudBaseäº‘å‡½æ•°è°ƒç”¨ç§‘å¤§è®¯é£API'}
        </div>
        <div>
          â€¢ æ”¯æŒä¸­æ–‡æ™®é€šè¯è¯†åˆ«ï¼Œ{useFallback ? 'è‡ªåŠ¨é€‰æ‹©æœ€ä½³è¯†åˆ«æ–¹æ¡ˆ' : 'å‡†ç¡®ç‡å¯è¾¾95%ä»¥ä¸Š'}
        </div>
        {useFallback && isWebSpeechSupported() && (
          <div className="text-blue-600">
            â„¹ï¸ å½“å‰ä½¿ç”¨æµè§ˆå™¨è¯­éŸ³è¯†åˆ«ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
          </div>
        )}
      </div>
    </div>
  );
}